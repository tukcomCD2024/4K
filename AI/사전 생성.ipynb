{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "TL_sentence_path = r'C:\\Users\\edcrf\\sentence_dataTL.csv'\n",
    "VL_sentence_path = r'C:\\Users\\edcrf\\sentence_dataVL.csv'\n",
    "\n",
    "# data파일 불러오기\n",
    "TL_sentence_data = pd.read_csv(TL_sentence_path, encoding='utf-8')\n",
    "VL_sentence_data = pd.read_csv(VL_sentence_path, encoding='utf-8')\n",
    "\n",
    "# 중복 제거, Pronuncication 열은 필요 없다고 생각\n",
    "TL_sentence_data.drop('Pronunciation', axis=1, inplace=True)\n",
    "TL_sentence_data = TL_sentence_data.drop_duplicates().reset_index(drop=True)\n",
    "VL_sentence_data.drop('Pronunciation', axis=1, inplace=True)\n",
    "VL_sentence_data = VL_sentence_data.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 형태소 분석기로 Okt를 사용\n",
    "import konlpy\n",
    "from konlpy.tag import Kkma, Komoran, Okt, Hannanum\n",
    "okt = Okt()\n",
    "\n",
    "standard_sentences = TL_sentence_data['Standard']\n",
    "dialect_sentences = TL_sentence_data['Dialect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 323371/323371 [22:18<00:00, 241.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# 방언 문장과 표준어 문장 각각 형태소 분석 진행\n",
    "standard_okt = []\n",
    "dialect_okt = []\n",
    "\n",
    "stop_words = ['이', '에', '는', '가', '도', '을', '뭐', '은','하고','게','에는','그', '를', '것', '으로','로']\n",
    "\n",
    "for i in tqdm(range(0, len(TL_sentence_data))):\n",
    "    standard_tokens = [token for token in okt.morphs(standard_sentences[i]) if token not in stop_words]\n",
    "    dialect_tokens = [token for token in okt.morphs(dialect_sentences[i]) if token not in stop_words]\n",
    "    standard_okt.append(standard_tokens)\n",
    "    dialect_okt.append(dialect_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['내일', '잔치', '있어서', '날', '많이', '추우면', '안', '될텐데', '내일', '많이', '춥다', '하더냐']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_okt[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['내일',\n",
       " '잔치',\n",
       " '있어가',\n",
       " '날',\n",
       " '마이',\n",
       " '추',\n",
       " '우마',\n",
       " '안',\n",
       " '델낀디',\n",
       " '내일',\n",
       " '많이',\n",
       " '춥다',\n",
       " '카더',\n",
       " '나']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect_okt[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 방언 문장과 표준어 문장을 각각 csv파일로 저장\n",
    "with open('st_stopwords_okt.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(standard_okt)\n",
    "\n",
    "with open('di_stopwords_okt.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(dialect_okt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv저장을 위해 리스트를 DataFrame으로 변환\n",
    "# standard_okt_df = pd.DataFrame(standard_okt)\n",
    "# dialect_okt_df = pd.DataFrame(dialect_okt)\n",
    "\n",
    "# standard_okt_df.to_csv(r'C:\\Users\\edcrf\\standard_okt.csv', index=False, sep =',')\n",
    "# dialect_okt_df.to_csv(r'C:\\Users\\edcrf\\dialect_okt.csv', index=False, sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv파일로 저장한 토큰화된 결과 불러오기\n",
    "# standard_okt_data = pd.read_csv(r'C:\\Users\\edcrf\\standard_okt.csv')\n",
    "# dialect_okt_data = pd.read_csv(r'C:\\Users\\edcrf\\dialect_okt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 토큰화 데이터 불러오기\n",
    "standard_okt_data = []\n",
    "dialect_okt_data = []\n",
    "\n",
    "with open('st_stopwords_okt.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        standard_okt_data.append(row)\n",
    "\n",
    "with open('di_stopwords_okt.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dialect_okt_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방언 문장의 토큰과 표준어 문장의 토큰 저장\n",
    "dialect_tokens = []\n",
    "standard_tokens = []\n",
    "\n",
    "for i in range(0, len(TL_sentence_data)):\n",
    "    for standard_word in standard_okt_data[i]:\n",
    "        standard_tokens.append(standard_word)\n",
    "    \n",
    "    for dialect_word in dialect_okt_data[i]:\n",
    "        dialect_tokens.append(dialect_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리스트에서 중복을 제거\n",
    "unique_standard_tokens = []\n",
    "unique_dialect_tokens = []\n",
    "seen1 = set()\n",
    "seen2 = set()\n",
    "\n",
    "for word in standard_tokens:\n",
    "    if word not in seen1:\n",
    "        unique_standard_tokens.append(word)\n",
    "        seen1.add(word)\n",
    "\n",
    "for word in dialect_tokens:\n",
    "    if word not in seen2:\n",
    "        unique_dialect_tokens.append(word)\n",
    "        seen2.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 토큰 수 >> standard: 5250260, dialect: 5467840\n",
      "중복 제거 후 토큰 수 >> standard: 91735, dialect: 111868\n"
     ]
    }
   ],
   "source": [
    "print(f\"중복 제거 전 토큰 수 >> standard: {len(standard_tokens)}, dialect: {len(dialect_tokens)}\")\n",
    "print(f\"중복 제거 후 토큰 수 >> standard: {len(unique_standard_tokens)}, dialect: {len(unique_dialect_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10718100\n"
     ]
    }
   ],
   "source": [
    "tokens = standard_tokens + dialect_tokens\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = standard_tokens + dialect_tokens\n",
    "\n",
    "unique_all_tokens = []\n",
    "seen3 = set()\n",
    "\n",
    "for word in all_tokens:\n",
    "    if word not in seen3:\n",
    "        unique_all_tokens.append(word)\n",
    "        seen3.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 방언과 표준어의 토큰 합친 수: 10718100\n",
      "중복 제거 후 방언과 표준어의 토큰 합친 수: 120136\n"
     ]
    }
   ],
   "source": [
    "print(f\"중복 제거 전 방언과 표준어의 토큰 합친 수: {len(all_tokens)}\")\n",
    "print(f\"중복 제거 후 방언과 표준어의 토큰 합친 수: {len(unique_all_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거하지 않았을 때\n",
    "### 중복 제거 x -> standard: 6536600, dialect: 6794654\n",
    "##### => 전체 토큰 수: 13331254\n",
    "### 중복 제거 o -> standard: 91751, dialect: 111884\n",
    "##### => 전체 토큰 수(중복 제거 x): 13331254, (중복 제거 o): 120152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 제거했을 때\n",
    "### 중복 제거 x -> standard: 5250260, dialect: 5467840\n",
    "##### => 전체 토큰 수: 10718100\n",
    "### 중복 제거 o -> standard: 91735, dialect: 111868\n",
    "##### => 전체 토큰 수(중복 제거 x): 10718100, (중복 제거 o): 120136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120136\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 중복을 하나도 제거하지 않았을 때의 단어 빈도수를 계산\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# 빈도수 높은 순서대로 정렬\n",
    "sorted_word = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "print(len(sorted_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안',\n",
       " '거',\n",
       " '때',\n",
       " '또',\n",
       " '고',\n",
       " '들',\n",
       " '많이',\n",
       " '우리',\n",
       " '좀',\n",
       " '다',\n",
       " '한',\n",
       " '저',\n",
       " '내',\n",
       " '나',\n",
       " '이제',\n",
       " '집',\n",
       " '에서',\n",
       " '사람',\n",
       " '잘',\n",
       " '그런']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어 빈도수가 높은 것들을 출력해보니 방언과 표준어의 관계와 전혀 관련없는 조사들이 많이 포함됨\n",
    "### 따라서 불용어 처리를 통해 제거가 필요함\n",
    "# stop_words = ['이', '에', '는', '가', '도', '을', '뭐', '은','하고','게','에는','그', '를', '것', '으로','로']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word: index + 1 for index, word in enumerate(all_tokens)}\n",
    "id_to_word = {index: word for word, index in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_id csv로 저장\n",
    "with open('word_to_id.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for word, idx in word_to_id.items():\n",
    "        writer.writerow([word, idx])\n",
    "\n",
    "# id_to_word csv로 저장\n",
    "with open('id_to_word.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for idx, word in id_to_word.items():\n",
    "        writer.writerow([idx, word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방언과 표준어 문장을 정수 인덱스로 변환\n",
    "dialect_indices = [[word_to_id[token] for token in tokens] for tokens in dialect_okt_data]\n",
    "standard_indices = [[word_to_id[token] for token in tokens] for tokens in standard_okt_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수화된 방언 문장 csv로 저장\n",
    "with open('dialect_indices.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for indices in dialect_indices:\n",
    "        writer.writerow(indices)\n",
    "\n",
    "# 정수화된 표준어 문장 csv로 저장\n",
    "with open('standard_indices.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for indices in standard_indices:\n",
    "        writer.writerow(indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
