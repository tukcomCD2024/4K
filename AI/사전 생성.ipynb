{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "TL_sentence_path = r'C:\\Users\\edcrf\\sentence_dataTL.csv'\n",
    "VL_sentence_path = r'C:\\Users\\edcrf\\sentence_dataVL.csv'\n",
    "\n",
    "# data파일 불러오기\n",
    "TL_sentence_data = pd.read_csv(TL_sentence_path, encoding='utf-8')\n",
    "VL_sentence_data = pd.read_csv(VL_sentence_path, encoding='utf-8')\n",
    "\n",
    "# 중복 제거, Pronuncication 열은 필요 없다고 생각\n",
    "TL_sentence_data.drop('Pronunciation', axis=1, inplace=True)\n",
    "TL_sentence_data = TL_sentence_data.drop_duplicates().reset_index(drop=True)\n",
    "VL_sentence_data.drop('Pronunciation', axis=1, inplace=True)\n",
    "VL_sentence_data = VL_sentence_data.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 형태소 분석기로 Okt를 사용\n",
    "import konlpy\n",
    "from konlpy.tag import Kkma, Komoran, Okt, Hannanum\n",
    "okt = Okt()\n",
    "\n",
    "standard_sentences = TL_sentence_data['Standard']\n",
    "dialect_sentences = TL_sentence_data['Dialect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 21.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# 방언 문장과 표준어 문장 각각 형태소 분석 진행\n",
    "standard_okt = []\n",
    "dialect_okt = []\n",
    "\n",
    "stop_words = ['이', '에', '는', '가', '도', '을', '뭐', '은','하고','게','에는','그', '를', '것', '으로','로']\n",
    "\n",
    "for i in tqdm(range(0, len(TL_sentence_data))):\n",
    "    standard_tokens = [token for token in okt.morphs(standard_sentences[i]) if token not in stop_words]\n",
    "    dialect_tokens = [token for token in okt.morphs(dialect_sentences[i]) if token not in stop_words]\n",
    "    standard_okt.append(standard_tokens)\n",
    "    dialect_okt.append(dialect_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여기는', '옥수수', '잘', '된다', '하던데', '옥수수', '말고는', '무슨', '농사', '많이', '짓습니까']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_okt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['여기는',\n",
       " '옥수',\n",
       " '갱',\n",
       " '잘',\n",
       " '된다',\n",
       " '카던디',\n",
       " '옥수',\n",
       " '갱',\n",
       " '말고는',\n",
       " '무신',\n",
       " '농사',\n",
       " '많이',\n",
       " '짓습',\n",
       " '니',\n",
       " '껴']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialect_okt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 방언 문장과 표준어 문장을 각각 csv파일로 저장\n",
    "with open('st_okt.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(standard_okt)\n",
    "\n",
    "with open('di_okt.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(dialect_okt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv저장을 위해 리스트를 DataFrame으로 변환\n",
    "# standard_okt_df = pd.DataFrame(standard_okt)\n",
    "# dialect_okt_df = pd.DataFrame(dialect_okt)\n",
    "\n",
    "# standard_okt_df.to_csv(r'C:\\Users\\edcrf\\standard_okt.csv', index=False, sep =',')\n",
    "# dialect_okt_df.to_csv(r'C:\\Users\\edcrf\\dialect_okt.csv', index=False, sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # csv파일로 저장한 토큰화된 결과 불러오기\n",
    "# standard_okt_data = pd.read_csv(r'C:\\Users\\edcrf\\standard_okt.csv')\n",
    "# dialect_okt_data = pd.read_csv(r'C:\\Users\\edcrf\\dialect_okt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 토큰화 데이터 불러오기\n",
    "standard_okt_data = []\n",
    "dialect_okt_data = []\n",
    "\n",
    "with open('st_okt.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        standard_okt_data.append(row)\n",
    "\n",
    "with open('di_okt.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        dialect_okt_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방언 문장의 토큰과 표준어 문장의 토큰 저장\n",
    "dialect_tokens = []\n",
    "standard_tokens = []\n",
    "\n",
    "for i in range(0, len(TL_sentence_data)):\n",
    "    for standard_word in standard_okt_data[i]:\n",
    "        standard_tokens.append(standard_word)\n",
    "    \n",
    "    for dialect_word in dialect_okt_data[i]:\n",
    "        dialect_tokens.append(dialect_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리스트에서 중복을 제거\n",
    "unique_standard_tokens = []\n",
    "unique_dialect_tokens = []\n",
    "seen1 = set()\n",
    "seen2 = set()\n",
    "\n",
    "for word in standard_tokens:\n",
    "    if word not in seen1:\n",
    "        unique_standard_tokens.append(word)\n",
    "        seen1.add(word)\n",
    "\n",
    "for word in dialect_tokens:\n",
    "    if word not in seen2:\n",
    "        unique_dialect_tokens.append(word)\n",
    "        seen2.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 토큰 수 >> standard: 6536600, dialect: 6794654\n",
      "중복 제거 후 토큰 수 >> standard: 91751, dialect: 111884\n"
     ]
    }
   ],
   "source": [
    "print(f\"중복 제거 전 토큰 수 >> standard: {len(standard_tokens)}, dialect: {len(dialect_tokens)}\")\n",
    "print(f\"중복 제거 후 토큰 수 >> standard: {len(unique_standard_tokens)}, dialect: {len(unique_dialect_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13331254\n"
     ]
    }
   ],
   "source": [
    "tokens = standard_tokens + dialect_tokens\n",
    "\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = standard_tokens + dialect_tokens\n",
    "\n",
    "unique_all_tokens = []\n",
    "seen3 = set()\n",
    "\n",
    "for word in all_tokens:\n",
    "    if word not in seen3:\n",
    "        unique_all_tokens.append(word)\n",
    "        seen3.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 방언과 표준어의 토큰 합친 수: 13331254\n",
      "중복 제거 후 방언과 표준어의 토큰 합친 수: 120152\n"
     ]
    }
   ],
   "source": [
    "print(f\"중복 제거 전 방언과 표준어의 토큰 합친 수: {len(all_tokens)}\")\n",
    "print(f\"중복 제거 후 방언과 표준어의 토큰 합친 수: {len(unique_all_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 제거 x -> standard: 6536600, dialect: 6794654\n",
    "#### => 전체 토큰 수: 13331254\n",
    "### 중복 제거 o -> standard: 91751, dialect: 111884\n",
    "#### 전체 토큰 수(중복 제거 x): 13331254, (중복 제거 o): 120152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120152\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 중복을 하나도 제거하지 않았을 때의 단어 빈도수를 계산\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# 빈도수 높은 순서대로 정렬\n",
    "sorted_word = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "print(len(sorted_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['고란',\n",
       " '노으면서',\n",
       " '싫니',\n",
       " '질퍽질퍽해',\n",
       " '씰어져',\n",
       " '뛰야',\n",
       " '땡긴다아이가',\n",
       " '묵어지겠지',\n",
       " '서가있을',\n",
       " '걸렸는거라',\n",
       " '보인데이',\n",
       " '올러오면',\n",
       " '보이겠나',\n",
       " '매져가',\n",
       " '축축해가',\n",
       " '찍을까',\n",
       " '것뱈',\n",
       " '골르라',\n",
       " '골라라면',\n",
       " '버려뻐려',\n",
       " '흘르니까',\n",
       " '해믹일꼬']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word[120130:120152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어 빈도수가 높은 것들을 출력해보니 방언과 표준어의 관계와 전혀 관련없는 조사들이 많이 포함됨\n",
    "### 따라서 불용어 처리를 통해 제거가 필요함\n",
    "# stop_words = ['이', '에', '는', '가', '도', '을', '뭐', '은','하고','게','에는','그', '를', '것', '으로','로']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
